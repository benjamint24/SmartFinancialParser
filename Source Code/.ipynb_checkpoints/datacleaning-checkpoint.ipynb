{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d5267d71-bafe-4d91-9035-39f4c7d17d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw rows:       1000\n",
      "Rows kept:            1000\n",
      "Date parse errors:    0\n",
      "Amount parse errors:  0\n",
      "Unmapped merchants:   0\n",
      "Unique merchants (27):\n",
      "   AIRBNB\n",
      "   AMAZON\n",
      "   APPLE\n",
      "   BEST BUY\n",
      "   CAR WASH PRO\n",
      "   CHIPOTLE\n",
      "   CITY UTILITIES\n",
      "   COSTCO\n",
      "   DOORDASH\n",
      "   GOOGLE\n",
      "   GYM MEMBERSHIP\n",
      "   HOME DEPOT\n",
      "   INSTACART\n",
      "   LOWE'S\n",
      "   LYFT\n",
      "   MACY'S\n",
      "   MCDONALDS\n",
      "   NETFLIX\n",
      "   OLIVE GARDEN\n",
      "   PANDA EXPRESS\n",
      "   SHELL\n",
      "   SPOTIFY\n",
      "   STARBUCKS\n",
      "   SUSHI HOUSE\n",
      "   TARGET\n",
      "   UBER\n",
      "   WALMART\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# input/output files\n",
    "INPUT_FILE = \"../datasets/synthetic_transactions.csv\"\n",
    "OUTPUT_FILE = \"../datasets/synthetic_transactions_clean.csv\"\n",
    "\n",
    "# merchants match my base merchants from data creation\n",
    "BASE_MERCHANTS = {\n",
    "    #first 16 should all map to the key\n",
    "    \"UBER\": [\"UBER\", \"Uber\", \"Uber Technologies\", \"UBER EATS\", \"UBER *TRIP\"],\n",
    "    \"STARBUCKS\": [\"Starbucks\", \"STARBUCKS\", \"Starbucks Coffee\"],\n",
    "    \"AMAZON\": [\"Amazon\", \"AMZN\", \"Amazon Marketplace\"],\n",
    "    \"WALMART\": [\"Walmart\", \"WAL-MART\", \"Walmart Supercenter\"],\n",
    "    \"TARGET\": [\"Target\", \"TARGET\", \"Target Store\"],\n",
    "    \"MCDONALDS\": [\"McDonalds\", \"McDonald's\", \"MCD\"],\n",
    "    \"SHELL\": [\"Shell\", \"Shell Oil\", \"SHELL GAS\"],\n",
    "    \"LYFT\": [\"Lyft\", \"LYFT RIDE\"],\n",
    "    \"SPOTIFY\": [\"Spotify\", \"SPOTIFY\", \"Spotify Pmnt\"],\n",
    "    \"NETFLIX\": [\"Netflix\", \"NETFLIX\", \"Netflix.com\"],\n",
    "    \"APPLE\": [\"Apple\", \"APPLE.COM/BILL\", \"Apple Services\"],\n",
    "    \"GOOGLE\": [\"Google\", \"GOOGLE *SERVICES\", \"Google Play\"],\n",
    "    \"DOORDASH\": [\"DoorDash\", \"DOORDASH\", \"DOORDASH*ORDER\"],\n",
    "    \"INSTACART\": [\"Instacart\", \"INSTACART\"],\n",
    "    \"AIRBNB\": [\"Airbnb\", \"AIRBNB\", \"AIRBNB PAY\"],\n",
    "    \"COSTCO\": [\"Costco\", \"COSTCO WHOLESALE\"],\n",
    "    \n",
    "    #last three should just map to the name in the end\n",
    "    \n",
    "    # Restaurants\n",
    "    \"OLIVE_GARDEN\": [\"Olive Garden\"],\n",
    "    \"CHIPOTLE\": [\"Chipotle\"],\n",
    "    \"PANDA_EXPRESS\": [\"Panda Express\"],\n",
    "    \"SUSHI_HOUSE\": [\"Sushi House\"],\n",
    "\n",
    "    # Retail\n",
    "    \"BEST_BUY\": [\"Best Buy\"],\n",
    "    \"HOME_DEPOT\": [\"Home Depot\"],\n",
    "    \"LOWES\": [\"LOWE'S\"],\n",
    "    \"MACYS\": [\"Macy's\"],\n",
    "\n",
    "    # Service\n",
    "    \"CITY_UTILITIES\": [\"City Utilities\"],\n",
    "    \"GYM_MEMBERSHIP\": [\"Gym Membership\"],\n",
    "    \"CAR_WASH_PRO\": [\"Car Wash Pro\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Date parsing and normalization\n",
    "# -----------------------------\n",
    "\n",
    "DATE_PATTERNS = [\n",
    "    \"%Y-%m-%d\",   # 2021-6-7 or 2021-06-07\n",
    "    \"%m/%d/%Y\",   # 6/7/2021 or 06/07/2021\n",
    "    \"%b %d %Y\",   # Jun 7 2021 or Jun 07 2021\n",
    "    \"%d-%m-%y\",   # 7-6-21 or 07-06-21\n",
    "    \"%d %b %y\",   # 7 Jun 21\n",
    "    \"%d %b %Y\",   # 7 Jun 2021\n",
    "]\n",
    " \n",
    "#regex using re that takes care of all MMM DD+suffix YY formats \n",
    "''' HAS to have (in order)\n",
    "        -starting (^) with three alphabetic characters mixed case allowed : (?P<mon>[A-Za-z]{3})\n",
    "        -1 more more spaces : \\s+\n",
    "        -date with one or two digits: (?P<day>\\d{1,2})\n",
    "        -required suffix right after: (st|nd|rd|th)\n",
    "        -1 more more spaces : \\s+\n",
    "        -year with 2 digits : (?P<year>\\d{2})\n",
    "        -NOTHING after: $\n",
    "\n",
    "    Example cases:\n",
    "        Jan 1st 21\n",
    "        Oct 23rd 19\n",
    "        Feb 07th 05\n",
    "        mAr 3rd 24\n",
    "        Apr   9th    17\n",
    "'''   \n",
    "_suffix_date_regex = re.compile(\n",
    "    \n",
    "    r\"^(?P<mon>[A-Za-z]{3})\\s+(?P<day>\\d{1,2})(st|nd|rd|th)\\s+(?P<year>\\d{2})$\"\n",
    ")\n",
    "\n",
    "# parse data with with datetime's dt which can automatically format all the other formats except for MMM Dth YY\n",
    "def parse_date(raw: str) -> Optional[str]:\n",
    "    s = str(raw).strip()\n",
    "\n",
    "    # Try all direct patterns first\n",
    "    for pattern in DATE_PATTERNS:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, pattern)\n",
    "            return dt.date().isoformat()  # YYYY-MM-DD\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # use regex to handle mmm dth yy\n",
    "\n",
    "    #match handles different starts\n",
    "    m = _suffix_date_regex.match(s)\n",
    "    if m:\n",
    "        #convert to valid datetime format to get ready to convert\n",
    "        mon_str = m.group(\"mon\")\n",
    "        day = int(m.group(\"day\"))\n",
    "        year_short = int(m.group(\"year\"))\n",
    "\n",
    "        year_full = 2000 + year_short\n",
    "\n",
    "        #convert to datetime\n",
    "        try:\n",
    "            dt = datetime.strptime(f\"{mon_str} {day} {year_full}\", \"%b %d %Y\")\n",
    "            return dt.date().isoformat()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Merchant normalization (27 uniques)\n",
    "# -------------------------------------\n",
    "\n",
    "# Canonical form for matching: uppercase and strip non-alphanumeric\n",
    "def _canonical_string(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", s.upper())\n",
    "\n",
    "\n",
    "# related to these should collpase to one of these 16\n",
    "BRAND_KEYS = {\n",
    "    \"UBER\",\n",
    "    \"STARBUCKS\",\n",
    "    \"AMAZON\",\n",
    "    \"WALMART\",\n",
    "    \"TARGET\",\n",
    "    \"MCDONALDS\",\n",
    "    \"SHELL\",\n",
    "    \"LYFT\",\n",
    "    \"SPOTIFY\",\n",
    "    \"NETFLIX\",\n",
    "    \"APPLE\",\n",
    "    \"GOOGLE\",\n",
    "    \"DOORDASH\",\n",
    "    \"INSTACART\",\n",
    "    \"AIRBNB\",\n",
    "    \"COSTCO\",\n",
    "}\n",
    "\n",
    "# Build (canonical_base, canonical_output) pairs:\n",
    "# - If family in BRAND_KEYS -> output key\n",
    "# - IF restaurants/retail/service -> output original name\n",
    "# CANONICAL_BASES = list of {OUTPUT AFTER CLEANING: KEY NAME}\n",
    "CANONICAL_BASES = []\n",
    "for family, names in BASE_MERCHANTS.items():\n",
    "    if family in BRAND_KEYS:\n",
    "        canonical_output = family          \n",
    "    else:\n",
    "        canonical_output = names[0]      \n",
    "\n",
    "    for base in names:\n",
    "        canon = _canonical_string(base)\n",
    "        if not canon:\n",
    "            continue\n",
    "        CANONICAL_BASES.append((canon, canonical_output.upper()))\n",
    "\n",
    "# Matches based on spelling errors using edit distance algorithm\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if not a:\n",
    "        return len(b)\n",
    "    if not b:\n",
    "        return len(a)\n",
    "    #list of counting integers to length of b, updates so it shows number of edits needed at each stage in word b\n",
    "    prev = list(range(len(b) + 1))\n",
    "    #loop to get min changes to turn a into b\n",
    "    for i, ca in enumerate(a, start=1):\n",
    "        cur = [i]\n",
    "        for j, cb in enumerate(b, start=1):\n",
    "            ins = cur[j - 1] + 1\n",
    "            delete = prev[j] + 1\n",
    "            sub = prev[j - 1] + (ca != cb)\n",
    "            #whichever takes the shortest\n",
    "            cur.append(min(ins, delete, sub))\n",
    "        prev = cur\n",
    "    #return final index at the end of b\n",
    "    return prev[-1]\n",
    "\n",
    "\n",
    "def clean_merchant(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns cleaned merchant name.\n",
    "    If it cannot be mapped, returns the string \"ERROR\".\n",
    "    \"\"\"\n",
    "   \n",
    "    s = str(raw).strip()\n",
    "    \n",
    "    if not s:\n",
    "        return \"ERROR\"\n",
    "\n",
    "    #uses cleaner regex function from earlier\n",
    "    canon_input = _canonical_string(s)\n",
    "\n",
    "    if not canon_input:\n",
    "        return \"ERROR\"\n",
    "\n",
    "    # first filter check for matches that have no spelling errors by handling extra characters and finding best match\n",
    "    best_match = None\n",
    "    best_len = 0\n",
    "    for canon_base, canonical_output in CANONICAL_BASES:\n",
    "        #checks if input contains one of the bases, only makes best match if has more matching characters than pervious best match\n",
    "        if canon_base in canon_input and len(canon_base) > best_len:\n",
    "            best_len = len(canon_base)\n",
    "            best_match = canonical_output\n",
    "    \n",
    "    # returns if found valid match\n",
    "    if best_match is not None:\n",
    "        return best_match\n",
    "\n",
    "    # second fiter accounts for spelling errors using previous levenshtein filter using the same length matching as filter 1\n",
    "    # matches when theres a very small difference due to spellingthere's\n",
    "    best_dist = 10**9\n",
    "    best = None\n",
    "    for canon_base, canonical_output in CANONICAL_BASES:\n",
    "        # gets number of matching, take canonical_output of smallest dist as small dist means least edits (likely 1 or 2 due to spelling errors)\n",
    "        dist = levenshtein(canon_input, canon_base)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best = canonical_output\n",
    "\n",
    "    # should return at this point if not at the first filter\n",
    "    if best is not None:\n",
    "        return best\n",
    "\n",
    "    # Final fallback â€” nothing matched\n",
    "    return \"ERROR\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Amount normalization, one functions\n",
    "# ---------------------\n",
    "\n",
    "# converts messy amounts and returns into 2 decimal format\n",
    "def parse_amount(raw: str) -> Optional[str]:\n",
    "\n",
    "    #leading and trailing spaces\n",
    "    s = str(raw).strip()\n",
    "\n",
    "\n",
    "    # Remove USD (rusing regex to match usd at beginning end and with mixed cases), $, and commas\n",
    "    s = re.sub(r\"(?i)usd\", \"\", s)\n",
    "    s = s.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "    s = s.strip()\n",
    "\n",
    "    #convert to float with two decimals\n",
    "    return f\"{float(s):.2f}\" if s else None\n",
    "\n",
    "\n",
    "# Main cleaning , bringing it all together\n",
    "# --------------------------\n",
    "\n",
    "def clean_csv() -> Dict[str, object]:\n",
    "    #keepign track of rows and errors\n",
    "    cleaned_rows: List[Dict[str, str]] = []\n",
    "    date_errors = 0\n",
    "    amount_errors = 0\n",
    "    merchant_unmapped = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    # create and fill new csv with clean values\n",
    "    with open(INPUT_FILE, mode=\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            total_rows += 1\n",
    "\n",
    "            # grabbing messy values\n",
    "            raw_date = row.get(\"date\", \"\")\n",
    "            raw_merchant = row.get(\"merchant\", \"\")\n",
    "            raw_amount = row.get(\"amount\", \"\")\n",
    "            \n",
    "            # cleaning values\n",
    "            clean_date = parse_date(raw_date)\n",
    "            clean_amount = parse_amount(raw_amount)\n",
    "            clean_merchant_val = clean_merchant(raw_merchant)\n",
    "\n",
    "            # error catching\n",
    "            if clean_date is None:\n",
    "                date_errors += 1\n",
    "                continue\n",
    "\n",
    "            if clean_amount is None:\n",
    "                amount_errors += 1\n",
    "                continue\n",
    "\n",
    "            if clean_merchant_val == \"ERROR\":\n",
    "                merchant_unmapped += 1\n",
    "\n",
    "            # adding clean data to dataset\n",
    "            cleaned_rows.append(\n",
    "                {\n",
    "                    \"date\": clean_date,\n",
    "                    \"merchant\": clean_merchant_val,\n",
    "                    \"amount\": clean_amount,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Write cleaned CSV\n",
    "    with open(OUTPUT_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=[\"date\", \"merchant\", \"amount\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(cleaned_rows)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_rows\": cleaned_rows,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"date_errors\": date_errors,\n",
    "        \"amount_errors\": amount_errors,\n",
    "        \"merchant_unmapped\": merchant_unmapped,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    #run clean_csv and print out statistics\n",
    "    result = clean_csv()\n",
    "\n",
    "    cleaned_rows = result[\"cleaned_rows\"]\n",
    "    total_rows = result[\"total_rows\"]\n",
    "    date_errors = result[\"date_errors\"]\n",
    "    amount_errors = result[\"amount_errors\"]\n",
    "    merchant_unmapped = result[\"merchant_unmapped\"]\n",
    "\n",
    "    print(f\"Total raw rows:       {total_rows}\")\n",
    "    print(f\"Rows kept:            {len(cleaned_rows)}\")\n",
    "    print(f\"Date parse errors:    {date_errors}\")\n",
    "    print(f\"Amount parse errors:  {amount_errors}\")\n",
    "    print(f\"Unmapped merchants:   {merchant_unmapped}\")\n",
    "\n",
    "    unique_merchants = sorted(set(r[\"merchant\"] for r in cleaned_rows))\n",
    "    print(f\"Unique merchants ({len(unique_merchants)}):\")\n",
    "    for m in unique_merchants:\n",
    "        print(\"  \", m)\n",
    "\n",
    "def cleaning_demo():\n",
    "    print(\"Now, we can clean the raw synthetic transactions file.\")\n",
    "    print(f\"Input file:  {INPUT_FILE}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(\"\\nProceed with cleaning? (y/n)\")\n",
    "\n",
    "    choice = input(\">> \").strip().lower()\n",
    "\n",
    "    if choice == \"y\":\n",
    "        result = clean_csv()\n",
    "\n",
    "        cleaned_rows = result[\"cleaned_rows\"]\n",
    "        total_rows = result[\"total_rows\"]\n",
    "        date_errors = result[\"date_errors\"]\n",
    "        amount_errors = result[\"amount_errors\"]\n",
    "        merchant_unmapped = result[\"merchant_unmapped\"]\n",
    "    \n",
    "        print(f\"Total raw rows:       {total_rows}\")\n",
    "        print(f\"Rows kept:            {len(cleaned_rows)}\")\n",
    "        print(f\"Date parse errors:    {date_errors}\")\n",
    "        print(f\"Amount parse errors:  {amount_errors}\")\n",
    "        print(f\"Unmapped merchants:   {merchant_unmapped}\")\n",
    "        \n",
    "        print(\"Cleaning completed successfully.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Cleaning canceled, quitting program.\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cd912e25-02a2-4027-8cb9-45663e30e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>merchant</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-26</td>\n",
       "      <td>AMAZON</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-11</td>\n",
       "      <td>STARBUCKS</td>\n",
       "      <td>2395.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>AIRBNB</td>\n",
       "      <td>1053.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>SHELL</td>\n",
       "      <td>1698.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>DOORDASH</td>\n",
       "      <td>1613.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   merchant   amount\n",
       "0  2021-09-26     AMAZON     1.00\n",
       "1  2024-08-11  STARBUCKS  2395.00\n",
       "2  2025-01-09     AIRBNB  1053.93\n",
       "3  2019-09-22      SHELL  1698.42\n",
       "4  2020-01-19   DOORDASH  1613.00"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(OUTPUT_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1085b854-0241-4a6f-957e-115d0575c6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UBER', 'UBER'),\n",
       " ('UBER', 'UBER'),\n",
       " ('UBERTECHNOLOGIES', 'UBER'),\n",
       " ('UBEREATS', 'UBER'),\n",
       " ('UBERTRIP', 'UBER'),\n",
       " ('STARBUCKS', 'STARBUCKS'),\n",
       " ('STARBUCKS', 'STARBUCKS'),\n",
       " ('STARBUCKSCOFFEE', 'STARBUCKS'),\n",
       " ('AMAZON', 'AMAZON'),\n",
       " ('AMZN', 'AMAZON'),\n",
       " ('AMAZONMARKETPLACE', 'AMAZON'),\n",
       " ('WALMART', 'WALMART'),\n",
       " ('WALMART', 'WALMART'),\n",
       " ('WALMARTSUPERCENTER', 'WALMART'),\n",
       " ('TARGET', 'TARGET'),\n",
       " ('TARGET', 'TARGET'),\n",
       " ('TARGETSTORE', 'TARGET'),\n",
       " ('MCDONALDS', 'MCDONALDS'),\n",
       " ('MCDONALDS', 'MCDONALDS'),\n",
       " ('MCD', 'MCDONALDS'),\n",
       " ('SHELL', 'SHELL'),\n",
       " ('SHELLOIL', 'SHELL'),\n",
       " ('SHELLGAS', 'SHELL'),\n",
       " ('LYFT', 'LYFT'),\n",
       " ('LYFTRIDE', 'LYFT'),\n",
       " ('SPOTIFY', 'SPOTIFY'),\n",
       " ('SPOTIFY', 'SPOTIFY'),\n",
       " ('SPOTIFYPMNT', 'SPOTIFY'),\n",
       " ('NETFLIX', 'NETFLIX'),\n",
       " ('NETFLIX', 'NETFLIX'),\n",
       " ('NETFLIXCOM', 'NETFLIX'),\n",
       " ('APPLE', 'APPLE'),\n",
       " ('APPLECOMBILL', 'APPLE'),\n",
       " ('APPLESERVICES', 'APPLE'),\n",
       " ('GOOGLE', 'GOOGLE'),\n",
       " ('GOOGLESERVICES', 'GOOGLE'),\n",
       " ('GOOGLEPLAY', 'GOOGLE'),\n",
       " ('DOORDASH', 'DOORDASH'),\n",
       " ('DOORDASH', 'DOORDASH'),\n",
       " ('DOORDASHORDER', 'DOORDASH'),\n",
       " ('INSTACART', 'INSTACART'),\n",
       " ('INSTACART', 'INSTACART'),\n",
       " ('AIRBNB', 'AIRBNB'),\n",
       " ('AIRBNB', 'AIRBNB'),\n",
       " ('AIRBNBPAY', 'AIRBNB'),\n",
       " ('COSTCO', 'COSTCO'),\n",
       " ('COSTCOWHOLESALE', 'COSTCO'),\n",
       " ('OLIVEGARDEN', 'OLIVE GARDEN'),\n",
       " ('CHIPOTLE', 'CHIPOTLE'),\n",
       " ('PANDAEXPRESS', 'PANDA EXPRESS'),\n",
       " ('SUSHIHOUSE', 'SUSHI HOUSE'),\n",
       " ('BESTBUY', 'BEST BUY'),\n",
       " ('HOMEDEPOT', 'HOME DEPOT'),\n",
       " ('LOWES', \"LOWE'S\"),\n",
       " ('MACYS', \"MACY'S\"),\n",
       " ('CITYUTILITIES', 'CITY UTILITIES'),\n",
       " ('GYMMEMBERSHIP', 'GYM MEMBERSHIP'),\n",
       " ('CARWASHPRO', 'CAR WASH PRO')]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CANONICAL_BASES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f134931-f67d-401d-9bf3-ba679b02d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee3e28-3f82-47b0-a349-ee6c542c7b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
